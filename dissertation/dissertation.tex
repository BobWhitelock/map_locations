\documentclass[12pt, a4paper]{report}
\usepackage[round]{natbib}
\usepackage{glossaries}
\usepackage{graphicx} 
\usepackage{float}

\title{Mapping Spatial References in Text using Google Maps}
\date{September 10, 2014}
\author{Bob Whitelock\\ MSc Software Systems and Internet Technology\\ Department of Computer Science\\ The University Of Sheffield}

%\makeglossaries
%\newglossaryentry{georeferencing}
%{
%	name=georeferencing
%	description={todo}	
%}


\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\tableofcontents

\glsaddall
\printglossaries

\chapter{Introduction}
% Briefly set scene of background to project

% Outline aims, objectives of project

% Summarize remaining chapters


\chapter{Literature Review and Previous Work}
% Overview of chapter contents and reasons structured this way

In this chapter we give an overview of the literature related to the different components of the geoparser system, as well as considering previous similar systems that have been constructed. We begin with section \ref{sec:overview_georeferencing}, discussing the importance of geoparsing as well as the general area of the georeferencing of information. We then discuss approaches to named-entity recognition in section \ref{sec_ner}, as this is an important component of any geoparsing system, before considering various other geoparsing systems in \ref{geoparsing_systems}.

% Review previous work and relationship to own
% Identify general trends and positions in area
% Overview of other available systems

% Background to georeferencing systems
\section{Overview of Georeferencing and Geoparsing}
\label{sec:overview_georeferencing}
% Overview of what is involved in a georeferencing system

% TODO redo this section

The aim of this project is to produce and evaluate a prototype a system for the recognition and identification of spatial references in text; such a system is known as a geoparser. A spatial reference here refers to any kind of reference to a location in space made in the text. This is not precisely defined but includes named locations, such as "London" or "Sheffield", as well as locations relative to a named location such as "5 miles east of Sheffield" and, depending on the definition used, includes less specific location references such as "a village" or "a city in England". The system will also include means of inputting text to be parsed and of viewing the output of the system on a map. In this section we give an overview of geoparsing and the procedure involved, as well as outlining the benefits of both geoparsing and the wider field of georeferencing, the association of geographic information with data. 

\citet{hill2006} describes the importance of georeferencing of information as a means for assisting its understanding and interpretation. References to location are ubiquitous in many types of text, for instance "[i]t has been estimated that at least 70 percent of our text documents contain placename references" (MetaCarta Inc., 2005a, cited in \citet[p.~5]{hill2006}), and so incorporating this in new ways into information systems will give benefits in many areas. [TODO add why is useful?]

Geoparsing in particular is a subject that both improved techniques and wider adoption would be of great use for both information retrieval and analysis, especially given the ever-increasing number of digital documents available on the internet. The development of improved geoparsing software and the wider use of this could have applications in many areas, both in simplifying current tasks and enabling new tasks that would be infeasible otherwise. For example, a geoparsing desktop application, browser plugin, or website could allow users to easily visualise the geospatial events in any document they are reading, aiding in their understanding and interpretation of it. Similarly, online content such as news articles already frequently includes maps to give readers a visual aid when reading the article, instead of manually creating these maps geoparsing software could could be used to automate this process. In addition the use of geoparsing software by either content creators or search providers could allow improved searching of content by geographic location, for instance by allowing the returning of relevant documents that contain references to locations in some area but do not reference the area itself, such as returning a document containing a reference to "Broomhill" (an area of Sheffield) when "Sheffield" is searched for, even if the document never mentions "Sheffield" itself. Finally, geoparsing of large bodies of text could enable analysis that would be impractical otherwise, such as analysing the references to location and their relations to each other in a large number of historical records.

\citet{hill2006} gives a detailed breakdown of the general steps involved in the geoparsing of a document, and these are as follows:

\begin{enumerate}
	\item {Digital text to be parsed is input to the system, and natural language techniques are used to identify features useful to the identification of spatial references, such as location names (by named-entity recognition), feature types (such as words like 'city' or 'mountain'), and other features of the text which may be useful (such as words indicating a relationship like "in" or "near" or directional or distance indicators like "north of" or "fifty miles east").}
	\item {The output of the initial step will be a number of spatial references, including but not necessarily limited to location names, and associated information from the text, and these should then be used to search for possible candidates for identification for each spatial reference, by lookup in a gazetteer (a geographic database containing location names as well as various associated information such as type of feature and geographic coordinates).}
	\item {If the gazetteer lookup is successful for a particular spatial reference then there will one or more candidates in the gazetteer for the identification of this reference.}
	\item {If more than one candidate has been found, an attempt must be made to disambiguate the most likely candidate to be that which the spatial reference is actually referring to; various techniques can be used for this, and both the various clues from the rest of the document and any other information given in the gazetteer can be used as factors in the disambiguation. If no candidates are found in the gazetteer for a spatial reference this disambiguation clearly cannot be done, and this is indicative of either limitations in the gazetteer or errors in the previous step of recognition of spatial references.}
	\item {Once a spatial reference has been identified as some gazetteer entry, coordinates can then be assigned to it with some degree of confidence. In addition, if desired an overall set of coordinates can be assigned to the document to georeference it as a whole; this could be done by different techniques, such as by an average of the individual identified coordinates or by identifying the location the document is primarily discussing and using the coordinates associated with this.}
\end{enumerate}


\section{Named Entity Recognition}
\label{sec_ner}
% What is involved, state of the art, Stanford CoreNLP in particular

One of the central components of a complete geoparsing system is a mechanism for the identification of spatial references in the text to be parsed. The most important part of this component will be a means of recognising location names, as these are the most prominent and unambiguous type of spatial reference; named locations must also be found before references in relation to these can be found. This problem is a specific instance of the general problem of named-entity recognition (NER), a major task in natural language processing (NLP). In this section an overview of NER is given with a summary of some of the main techniques in the field.

The task of NER is the automatic identification of parts of text that are the name of some named-entity, such as of a person, a location, or an organization, as well as for the classification of identified named-entities into groups such as these. What precisely is defined as a named-entity is not definitive, however for the most part the definition of \citet{conll2002} will suffice, that '[n]amed entities are phrases that contain the names of persons, organizations, locations, times and quantities'. NER broadly consists of two parts, firstly the recognition of a sequence of tokens as a named-entity of any type, and secondly the classification of named entities into particular types. It is by no means a simple task for many reasons, including that there are many different situations in which a named-entity can appear, the exact same string can refer to one type of entity in some situations and another in a different situation (for example "England" could refer to the country England or the organization of the English football team, depending on context), and difficulties in determining what constitutes a single named-entity (for example "The Royal Bank of Scotland", which would correctly be identified as an organization, could easily be interpreted as two separate entities, an organization and a location, separated by "of" - this is due to the emphasis normally placed upon capitalization in determining named entities).

NER has received a considerable amount of work over the past twenty years, with a variety of techniques developed, many with considerable success in the particular area targeted. Techniques use various features of words, the document generally, and external sources to try to determine both the extent of and classification of named-entities in the document, by the development of some rules for recognition and classification based on these features. Such features can be of a variety of types and can take different types of values, such as boolean values, one of a range of valid values, or a string. As described by \citet{nadeau2009}, common features used for recognition and classification of named entities can include:

\begin{itemize}
	\item {Word-level features: features of the characters of an individual word in the document being processed, including: case (upper, lower, starting with a capital), internal punctuation and digits or other characters in the word, the morphology of the word, the part-of-speech the word is determined to be (the type of word, whether a foreign word), and the application of some function over the word (such as extracting the non-alphabetic characters of the word).}
	\item {List-lookup features: these are various external sources to be used to assist in the recognition and classification of named-entities, including: dictionaries (e.g. to identify common nouns), lists of stop words, lists of common capitalized nouns, and common abbreviations, lists of various entities, and lists of cues for entities (such as words typically found in organization names, person titles or common prefixes, and common words found in different entity names).}
	\item {Document features: features of the overall context of a word, the whole document, or the corpus as a whole, including: features of other occurrences of a word (such as their case, and properties of co-references of a word), the other words in the context of the word, any meta information for the document (such as the url or information from tables or figures in the document), and the frequency of words and phrases occurring in the document, and how often certain words occur together.}
\end{itemize}

Approaches to NER then use these various features to try to identify and classify named-entities in particular documents, usually by the  development of both recognition and classification rules which, applied to a document, should identify and classify named-entities based on the features of the document. As described by \citet{nadeau2009}, early approaches involved the use of "heuristics and hand-crafted rules" \citet[p.~2]{nadeau2009}, specifically tailored by the creator to try to tackle typical situations, determined by inspection of text in the domain to be tackled, and then applied by manually creating regular expressions to match these rules. More recently, however, the most common approach is to use various supervised machine learning techniques to train either a rule-based system or a sequence labelling algorithm, using a large quantity of human-annotated text as the training data. Some techniques in this approach include Hidden Markov Models, Decision Trees, Maximum Entropy Models, Support Vector Machines and Conditional Random Fields; the details of the majority of these is beyond the scope of this section.

There are also other techniques which require less or no human-annotated text to perform; these are important areas of development due to the time and expense needed to annotate new datasets for training use in purely supervised machine learning approaches. One such approach is semi-supervised learning, the main technique of which is known as 'bootstrapping'. This involves the use of a smaller dataset than with a purely supervised learning approach as seed example data to start the learning process, and then searches for similar examples to those in this set to reinforce the learning; some such approaches have had success close to a purely supervised approach. Finally there are unsupervised approaches, the main technique of which is known as 'clustering', which require no training data. These techniques mostly involve the use of lexical patterns and statistics on an unannotated corpus to enable recognition and classification of named-entities.

Given the advanced state of the NER field coupled with the limited nature of this project, we decided to incorporate already available software to initially fulfil this part of the system. This  will enable concentrating on implementing more novel aspects of the system rather than expending the limited resources of the project on reimplementing what is already available.

A number of free and open source NER systems are available which could be used for this purpose. In particular these include Stanford NER \citep{finkel2005} which [TODO reasons chosen]

Of course for a geoparsing system to be comprehensive in its coverage of spatial references the identification of just named locations is not enough, even if this could be done with perfect accuracy, as there are many other types of spatial references that may be made in written text. To name just some these include:

\begin{itemize}
	\item { References relative to some named location, for example "fifty miles east of Sheffield". }
	\item { References to some unnamed location, which could potentially be determined from the context, such as to "the capital of Spain".}
	\item { Co-references to a previously mentioned named location or other spatial reference, for example [TODO]}
	\item { An absolute reference to some geographic location using some formal georeferencing notation, for instance by latitude and longitude in some format such as "53.3836° N, 1.4669° W" or "(53.3836, -1.4669)" (both coordinates for Sheffield, England). }
\end{itemize}

As can be seen there can be many kinds of spatial reference beyond just the names of named locations. However incorporating any of these into a geoparsing system would require some amount of additional work on top of just using a NER system. Again due to the limited resources available for the project it was decided initially to build a full geoparsing system using just an NER system for the recognition of spatial references, namely Stanford NER. Once this was completed improving the recognition component of the system could be done, and this would be one way to improve the overall coverage of the geoparser. As it turned out we decided to concentrate our time on improving the disambiguation aspect of the system while keeping Stanford NER as the recognition component of the system, and so this direction of the project was not taken; this would therefore be one good extension to the project. [TODO remove]

\section{Geoparsing Systems}
\label{sec:geoparsing_systems}
% Discuss various georeferencing systems and algorithms involved

TODO GOT UP TO HERE WHEN REVIEWING ADD TO THIS SECTION

A number of other geoparsing systems have been constructed in the past few years, including research prototypes not intended for widespread use as well as industrial projects both proprietary and open-source. In this section we give an overview of a number of these, outlining the approaches taken by each.

\subsection{Research Systems}
\label{subsec:research_systems}

One recent research project that had a reasonable amount of success at building a geoparsing system was the MIPLACE system created by a group at the MITRE Corporation \citep{mani2010}, a project to develop an automatic tagger for the SpatialML annotation scheme \citep{spatialml2009}. SpatialML is an XML schema specifically designed for the comprehensive annotation of spatial references in text, as well as any textually indicated relationships between spatial references. It includes a 'PLACE' tag to be used for marking up references to a specific location, covering both named references ("Sheffield") and nominal references ("a city"); it is also worth noting that this tag includes a 'gazref' attribute for the specific association with a spatial reference of a gazetteer entry from some gazetteer. The scheme also includes a 'SIGNAL' tag for marking up specific spatial indicators such as "in" or "near", and non-consuming 'LINK' and 'RLINK' tags for indicating given topological (e.g. containment or adjacency) or trajectory (e.g. distance and direction) relationships respectively between  spatial references.

The SpatialML automatic annotator's major components follow the general structure for a geoparser as set out in \ref{sec:overview_georeferencing}, consisting of an 'Entity Tagger' followed by a 'Disambiguator' for the recognition and disambiguation of spatial references. In addition it also includes a final 'Relations Tagger' for adding in 'RLINK' and 'LINK' tags to a tagged document, and the ability to take HTML documents as input as well as to transform the SpatialML output into KML (Keyhole Markup Language) for viewing in map viewing software such as Google Earth. Both the Entity Tagger and the Disambiguator use supervised machine learning techniques to be trained to fulfill their purposes. They use feature vectors composed of various features of the document and the gazetteer, such as is described in \ref{sec_ner}; the data used to train the components is a human-annotated SpatialML corpus also produced as part of the project. [TODO more detail]

The MIPLACE system was evaluated against a ASC and ProMED SpatialML annotated corpora, for both recognition and and disambiguation accuracy. Recognition of spatial references was evaluated by considering the F-measure of the extent of references tagged, a binary decision which is successful only if the exact same text is identified as a spatial reference as in the corpus. For this the system scored F-measures of 86.9 and 67.54 on the ASC and ProMED corpora respectively. A number of reasons are given [TODO more]

[Refactor not very good] Despite the successes of the MIPLACE project, a number of ways it could be approved as a geoparser can be observed. For example, the method used to recognise spatial references is developed especially for this system, while the field of NER is quite advanced with sophisticated systems developed using means beyond the scope of just the MIPLACE project. To some extent it would appear necessary to have a custom built component to the disambiguator to retain the same functionality as MIPLACE currently, due to the tagging of nominal spatial references such as "a city". However, given that sophisticated and usable specialized NER systems have been developed, it would seem that gains to the recognition component of the system could be made by incorporating a specialist NER system into the recognition component rather than reimplementing this as part of the geoparser. Also with the disambiguation component of the system there is not a great deal of detail given about the particular machine learning algorithm used, however it is reasonable to suppose that this could be improved upon, especially given the many approaches taken to the subject in recent years [insert citations].

However there are a number of barriers to the MIPLACE system being a base geoparser to be improved upon. Firstly, while the source code of the system is available \footnote{http://sourceforge.net/projects/spatialml/}, it is undocumented in many ways, for example no documentation of the overall architecture of the system given and there is a minimal amount of comments within the code itself. In addition the system was observed to require a non-trivial amount of setup in a new environment in order to use. Furthermore, the gazetteer used for the disambiguation component of the system is the IGDB \citep{igdb2005}, which is not publically available. These factors combined mean that a considerable amount of effort would need to be expended to adapt the system in its current state in order to further develop it as a geoparser. Added to this, the MIPLACE system is under copyright to the MITRE Corporation so it is unclear if it would even be possible to pursue further development of the system. All these reasons together imply that, while MIPLACE is reasonably successful as a geoparser, there is still a need for a robust, documented, open-source and extendable geoparser in order to...? TODO

Another project to build a complete geoparser was undertaken by \citet{tobin2010}... [TODO explain]

[TODO add about that other project]

\subsection{Commercial Systems}
\label{subsec:commercial_systems}

A number of commercial geoparsers have also been developed. These include the GeoTag function of MetaCarta's GSRP \footnote{http://www.metacarta.com/products-platform-geotag.htm} and Yahoo BOSS PlaceSpotter \footnote{https://developer.yahoo.com/boss/geo/docs/key-concepts.html}. GeoTag is described as a 'production-level geographic entity resolving function that parses content, extracts geographic references, and resolves the geographic meaning intended by the author', while PlaceSpotter is intended for 'identifying places in unstructured and "atomic" content ... and returning geographic metadata for geographic indexing and markup'. Both of these are commercial web services, and so of course are also closed source and little explanation is given about the procedure involved in either service, nor is any details of the success rate of either system given. As closed-source, commercial systems 
 
%Add?: Of course as a prototype system developed by a corporation it is understandable that the MIPLACE system would have these

\subsection{CLAVIN: An Open Source System}
\label{subsec:clavin}

Finally, an open source geoparsing package is CLAVIN \footnote{https://github.com/Berico-Technologies/CLAVIN} (meaning Cartographic Location And Vicinity INdexer). This system was recently developed and released in 2013 by Berico Technologies, and consists of a Java library that can be used for the easy geoparsing of any text. CLAVIN uses the Apache OpenNLP \footnote{https://opennlp.apache.org/} library for the recognition of location names, and the GeoNames \footnote{http://www.geonames.org/} gazetteer for obtaining candidates for recognized names, using fuzzy matching and considering alternate names for locations in addition to searching for direct matches.

The system uses manually created heuristic rules for the disambiguation of candidates, taking the context of the document into account. While no full evaluation of the system has been made, it was observed by observation to perform well on a number of documents, correctly identifying most locations mentioned.

We only became aware of this project quite late in the development of our own system, and given that it is a robust 

\chapter{Project Overview}
% Outline chapter: because more experimental project merge requirements and overview of design into this section, as requirements quite flexible and process more iterative than fixed requirements-design-implementation

In this chapter we give a high-level overview of various major aspects of the geoparser project. This structure is followed rather than splitting the different sections of the project into a strictly waterfall-like series of steps due to the more experimental and iterative nature of this project. As such there were less core requirements for the project, and the direction taken with the project was more open to adaptation as the project developed. This chapter begins with outlining the central requirements for the project, along with various possible optional directions it could proceed in. Next we give a brief discussion of the choice of programming language to be used for the project, before moving on to give a high-level overview of the architecture of the system in light of both the necessary requirements and possible extensions. Finally we give an overview of the evaluation procedure to be used for evaluating the success of the project.

\section{Requirements}
\label{sec:requirements}
% Overview of requirements and aims of system in light of literature review

% by 10.30 - edit and complete!

In this section we outline the requirements the geoparsing system should have in order to be considered complete and a success. Although this project should result in the production of a reasonably complete and functional prototype geoparser system, it is also of a somewhat experimental nature and could be developed in numerous directions, and so has few core requirements. As such our discussion here will be of a discursive nature, outlining the core requirements of the system along with a discussion of various possible options and extensions to these, where this is applicable. 

%As such the requirements for the project are fairly fluid, and so the requirements outlined here consist of a number of categories. Firstly there are core requirements for the construction of the geoparser, without which the project could not be considered complete. In addition we also outline other requirements we have chosen for various reasons, in terms of the architecture and additional features to be included in the system.

% put about requirements discursive rather than quantitative

The core of any geoparsing system must follow a structure similar to that set out in \ref{sec:overview_georeferencing}, with reference to \citet{hill2006}. Following this, first of all some means of inputting text to be parsed by the system is needed. The simplest way to enable this would be to allow the system to take text files as input directly; this would enable easy processing by the system of any text file, so would be useful as a means of general input of text to be processed to the system.

While file input alone would be enough to enable users of the system to process any text, it would also be very convenient to allow the parsing of HTML web pages for their main content and allowing this to be used as input for the system. This would be especially useful as the web is the most common way for people to read new digital documents, such as news articles, that may make references to numerous locations that they may not be familiar with, and so this is a situation where a geoparser would be of great aid to their understanding of many documents. As such it would be advantageous for the geoparser as a useful system to be able to take as input the URL of some HTML document on the Web, and then have a preliminary step of extracting the main textual content from this page before processing this through the remainder of the system.

After text has been input to the geoparser the next stage in the process is the recognition of spatial references in the input text. This could be taken to varying levels depending on the extent of spatial references it is desired to identify, with there being two aspects to how effective this component of the system is overall at recognition of spatial references. Firstly, the accuracy of recognition, i.e. how accurately recognised spatial references correspond with the true geographic location intended by the author, is of course of key importance.

In addition the scope of recognition is important, meaning the number of forms of spatial references that are recognised by the system. The key type of spatial reference to include is that of directly mentioned named locations, such as "London" or "Paris". As such the core component of this stage should be the NER of location names. As explained above this is a field with extensive previous work, including the availability of open-source NER systems such as Stanford NER, which will be used in this component of the system. In addition to just named locations there are many other kinds of spatial references which recognition could be included for, some of which are summarized in \ref{sec_ner}.

The recognition of all of these types of references is more specialized to the building of a geoparser, whereas NER is a more general and developed field with existing systems that can be used. Therefore the inclusion of provision for these in the recognition component of the system would require some amount of additional work, both to add support for recognition of these references as well as to add the disambiguation and plotting of other forms of spatial reference. As such, it was decided that initially the system should just include the recognition of named locations using Stanford NER, with the possibility for later expansion to include other types of spatial reference.

Once spatial references have been identified in the input text, the next stage in the geoparsing process is the identification of candidate locations for these. In order to do this some knowledge base is needed as a population to select candidates from; for this purpose a gazetteer, a geographic database, is needed in the system. The selection of the gazetteer can have a significant effect on the effectiveness of the geoparser. Choosing a gazetteer with good scope of coverage can improve the number of candidates found for a particular spatial reference and so the likelihood that the location which is really being referred to is present in the list. In addition choosing a gazetteer which provides as much information as possible about the different location entities means that there is more information to work with during the disambiguation stage of the geoparsing process. More information on the choice of gazetteer as well as the interface to this will be given in \ref{subsec_gazetteer}.

After a number of candidates for each spatial reference have been selected, the disambiguation process can be performed on these in order to select the most likely to be the true location. This is an important step of the process as there may be a considerable number of candidates found in the gazetteer for any particular spatial reference, only up to a few of which could possibly be considered correct, while all others are incorrect. Note that we say here that a few could potentially be correct rather than just one as it is sometimes the case that there are multiple gazetteer candidates which can be considered to refer to the same physical location.

The disambiguation process should consist of some procedure for selecting the most likely (if any) of the candidates to be true, and will rely on information similarly to that used in NER, as described in \ref{sec_ner}. Such information includes:

\begin{enumerate}
	\item { The context of the spatial reference itself, such as the words used within some distance of it, in particular other spatial references nearby and the context of these, e.g. in "London, England" the comma indicates containment of "London" within "England", and so incorporating this information into the disambiguation process would help the recognition of "London" as London, the capital of England rather that London, Ontario or some other London.  	}
	\item { Other recognized named entities could also be used to aid in the disambiguation process, for instance if a person found is often associated with a particular location and that location is a candidate for a particular spatial reference, that candidate is more likely to be the true location than it would be otherwise.
	} 
	\item { Information from the gazetteer can also be used, for instance if there is some indication from its context of a spatial reference's type (such as a city or mountain), candidates from the gazetteer which share this type can be favoured. 
	} 
	\item { Further sources could also be used, as can be done for NER, such as using Wikipedia \footnote{http://en.wikipedia.org} to determine more information about candidates to use for the disambiguation.
	}

\end{enumerate}

More information on possible approaches to the disambiguation process using this information will be given in [TODO reference]

Finally, at the end of the disambiguation process the result will be a text document along with certain identified spatial references in the document, with each of these associated with (at most one) physical location as given in a gazetteer. Given that an important use of a geoparser is to aid readers in their understanding of text that includes spatial references, an essential part of the system should be a way to visualize these references. Therefore the final component of the system should be the generation of a map containing the final recognised locations for the user to view. The map interface could be taken to various extents...
 
% while not part of system is necessary to have means of evaluating system
In addition to the system itself as described here, it is important due to this project's experimental nature to have some means of evaluating the system overall as well as its main components, to determine how well the system is performing and how this varies as it is altered. As such we will also need to construct an effective evaluation procedure to be used for this. Further details of the steps involved in this will be explained in \ref{evaluation_procedure}. 

\section{System Architecture}
% Overview of architecture of system as series of components with overall pipeline through components, reasons and purpose for design

% these subsections should maybe be sections? wait til later to refactor

\subsection{Overall Architecture}
\label{overall_architecture}

% explain how architecture follows from requirements

In this section we now describe the system architecture as resulting from the requirements as discussed above; this architecture can be seen in \ref{fig:geoparser_architecture}.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.5]{geoparser_architecture}
  \caption{Architecture of Geoparser System}
  \label{fig:geoparser_architecture}
\end{figure}

As can be seen from the diagram, the system has been split out into distinct modules, each taking input from the previous module and performing a major task in the geoparsing system, before giving output suitable for the next module. This modularity is designed to allow the components of the system to function as independently as possible from each other and so allow them to be developed as distinct units, each performing its own task. It should also make it easier for modules to be swapped out in favour of new components from other sources, or reused in other contexts, in either case with perhaps just a small amount of modification of the input and output.

The first such module then is for the parsing of HTML in order to extract the main content (as seen from a web browser) that a user is likely to want to have geoparsed. This module is optional and is needed so that users can input the URL of a web page to geoparse to the system. From the diagram it can be seen that this module makes use of the Readability Parser API, further information on this and the reasons for its choice can be found in \ref{content_acquisition}.

Alternatively users can input a text file to the system, in which case the contents of the file will be the text to be geoparsed. In either case, the resulting text will be input to the Spatial Reference Recognition module. The purpose of this module is the recognition of as many spatial references in the input text as possible, and it makes use of Stanford NER as a core component in order for this. It should then output the resulting text with the spatial references identified in a format suitable for the Spatial Reference Identification module to process.

This identification module has two main components. First of all it needs the capabilities of obtaining candidates for each input spatial reference from a gazetteer.  For this purpose the GeoNames gazetteer has been chosen, accessed using a local copy of the gazetteer in a MySQL database. The reasons for the choice of this gazetteer and means of access are explained in \ref{subsec_gazetteer}. Once these are obtained the module then needs a disambiguation procedure to select the best candidate to take as the true location; the implementation of this is described in [TODO ref], while further possibilities are discussed in [TODO ref].

Finally there is the Map Generation module [TODO change name in diagram], which takes these identified gazetteer candidates as input, and generates a map displaying these. In order to generate a map we need to use some mapping file format that can be easily generated and viewed in a mapping program. For this purpose we have use Keyhole Markup Language (KML) \footnote{https://developers.google.com/kml/documentation/}. This is an XML format providing an easy mark-up scheme for annotations to maps of the Earth, such as for marking locations to place a location marker on. KML files can be used to add mark-up to Google Maps, making this an ideal choice for having a final map output from the system that is viewable in Google Maps.

Once KML is generated for the identified locations, it must then be used to produce the final map to be output by the system. For the viewing of the map we use Google Maps, as this provides an easy to use map interface that is easy to incorporate into a HTML page. Therefore, after the KML has been created a HTML page must then be generated, including a Google Maps map containing the location markers as specified in the KML; this page will be the final output of the system.

\subsection{Choice of Programming Language}
% Brief reasoning for choosing Python as main programming language for project

In this section we describe the reasons for choosing Python as the programming language for the implementation of the majority of the project.

The choice of programming language or languages to be used for implementing any particular system is an important decision to be made before the implementation of the system can be undertaken. There are many different programming languages in active use today, and a myriad of factors in deciding which to use for the implementation of any particular project. Each programming language has had design decisions made in its development to make it more suitable for tackling different problem domains, as well as having different features in terms of which paradigms it supports and the expressiveness of its syntax. In addition to the core language, the capabilities of both the standard and available third-party libraries for the language will greatly effect the suitability of a language for a project, as the lack of library support for some feature may mean it will need to be implemented as part of the project when it would not otherwise.

Often one of the key factors effecting whether a language will be used however is only tangentially related to the language's design, as the capabilities of those undertaking the project must be taken into account. As such, as the sole developer there are only two languages I would feel capable of implementing the system in without having to sacrifice efforts in learning a new language that would be better spent elsewhere, and these are Java and Python.

Both of these languages are well-established with both having sophisticated standard and third-party libraries, as well as both being extensively used in NLP. One reason to favour Java as the primary language is that all of the Stanford CoreNLP tools, including Stanford NER which is to be used in the spatial reference recognition component of the system, are programmed in Java. Therefore use of Java would enable the calling of this code directly from the system. However, the CoreNLP tools are also runnable externally from the command line, and so it would not be strictly necessary to use Java code to interface with them (as it turned out it was in fact needed to write a small amount of Java code to interface with CoreNLP, which could be access using Python; see \ref{sec_spatial_reference_identification} for more details).

On the other hand, there are many reasons to favour Python for a project such as this. Python has an expressive syntax with many simple idioms for which the equivalent Java code is far more verbose, such as list comprehensions and first-class functions; this means it is often easier to manage complex logic due to less code being needed to express it. In addition its dynamic nature make it ideal for a more experimental project such as this, which is also not intended to be of such a size that the benefits of Java for large projects, such as type safety, would outweigh these advantages. For these reasons Python was chosen as the major implementation language for the project, as its expressiveness will allow the project to progress quickly and with less boilerplate code. 

\subsection{Choice of Gazetteer}
\label{subsec_gazetteer}
% Factors involved in choosing a gazetteer and reasons for choosing

% move to impl chapter

As explained above, the choice of gazetteer to be used for drawing the candidate locations for a spatial reference from is an important one, effecting the likelihood that the correct location for any given spatial reference will be present, as well as the amount of information available on locations and so available to be used for disambiguation. There are a number of key criteria that should be taken into account for the selection of a gazetteer. As described by \citet{leidner2004} these include:

% finish list

\begin{enumerate}
	\item { Gazetteer availability: free resources should be favoured in research as they allow the sharing of data. This is true also for this project as the use of a free gazetteer will allow the free use of the system; additionally this is necessary as there is no budget for the project. Also related to this, some resources are free but do not have data available in a convenient format or at all, and/or do not have sufficient documentation describing or for accessing the data.
	}
	\item { Gazetteer scope: gazetteers can range in scope from local through regional and national to global, and a scope of at least the area that is being investigated is needed in order to provide full coverage. The system to be built is intended as a worldwide geoparser, and so a gazetteer with global coverage is necessary for this. 
	}
	\item { Gazetteer completeness: gazetteers range in their comprehensive coverage of location names; for this project in particular it is clearly superior to have as comprehensive coverage as possible to increase the likelihood that a corresponding entry is present for any recognised location. 
	}
	\item { Gazetteer correctness: geographic and administrative names used change over time, combined with the fact that data can be input incorrectly in the first place, means that any gazetteer of significant size will contain wrong or outdated information; obviously it should be desired to minimize this as much as possible, all other things being equal.
	}
	\item { Gazetteer granularity: gazetteers can range in the coarseness of the locations they include, with each having some threshold size and relevance of locations which will be included. As Leidner observes, "[a] less fine-grained gazetteer might
actually facilitate the toponym resolution task by pro-
viding a useful bias", by reducing the noise of irrelevant candidates which will be very rarely correct \citep{leidner2004}. However, occasionally smaller and less well known locations will be mentioned, and if they are not included in the gazetteer in the first place there is no chance of resolving them; this therefore emphasizes the need for an effective disambiguation process, that filters out these locations unless there are strong reasons to consider them. 
	}
	\item { Gazetteer balance: gazetteers can also vary in how uniform the granularity and correctness they provide over different regions is; more uniformity should generally be favoured, however not at the expense of other factors.
	}
	\item { Gazetteer richness of annotation: the additional information provided with a location name can vary between gazetteers, from just latitude and longitude to detailed information on the type of location and relationships to other locations; for this project more detail should be favoured over less as this is more information to use for the disambiguation process, and some properties such as population are particularly important as they are likely to have a strong positive correlation with the frequencies of a location being mentioned in text.
	}
	
\end{enumerate}

% number of different gazetteers available - discuss a few of them and why chose GeoNames

A number of gazetteers were evaluated for their suitability for use in the system, bearing the above considerations in mind. One gazetteer considered was the NGA GEOnet Names Server (GNS) \footnote{http://earth-info.nga.mil/gns/html/}, an official US database of over 5 million features and 9 million feature names. However this database only includes names of locations that are not in the U.S., U.S. territories, or Antarctica, and so these location names would have to be incorporated from other sources, such as the  Geographic Names Information System (GNIS) \footnote{http://geonames.usgs.gov/domestic/index.html}.

Other gazetteers were also considered, however all were lacking in various aspects, including scope, completeness, and particularly the easy availability of their data and documentation for it.

However one gazetteer was clearly the best choice in all of the categories considered, this is the GeoNames gazetteer \footnote{http://www.geonames.org/}. This is a freely available gazetteer consisting of over 10 million geographic names and over 9 million unique features, with many points of information about each, such as population and both a general and specific feature code for the type of feature.

The data for GeoNames is also drawn from many sources, including both the GNS and GNIS, as well as numerous other countries' geographic data sources and other web sources \footnote{http://www.geonames.org/data-sources.html}. As such it is clearly a better choice than using any or a few of these sources independently.

Finally, GeoNames provides both a well-documented web API \footnote{http://www.geonames.org/export/ws-overview.html} as well as a daily database dump of the entire gazetteer (http://www.geonames.org/export/).

Once GeoNames had been chosen as the gazetteer to be used, it remained for one of these to be chosen as a means for using the gazetteer. The database dump was chosen to use as the gazetteer would need to be accessed many times for every document to be geoparsed (at least once for each spatial reference given), and so the use of a web API would greatly slow down the process of finding candidates due to having to wait for requests to be responded to. The raw GeoNames data was imported into a MySQL database using a freely available script for this purpose \footnote{http://forum.geonames.org/gforum/posts/list/6703.page}, with new indices later added to the database to optimize it for the queries needed.

%such as the Getty Thesaurus of Geographic Names \footnote{http://www.getty.edu/research/tools/vocabularies/tgn/} and

% Creating and indexing database - include footnote to script used % quickly add this

\section{System Evaluation Procedure}
\label{evaluation_procedure}

% Description of procedure to be used for evaluating the effectiveness of the overall program and disambiguation strategies

As an experimental system, an evaluation procedure is needed for evaluating how accurate the system and its various parts are and how this changes as aspects of the system and its components are changed. This is necessary to evaluate the success of the system overall as well as to be able to alter parts of the system, such as the disambiguation algorithm, and objectively see the effect of this on the system's performance.

% Further to requirements for good evaluation procedure
% Overview of evaluation - to tell if results correct need to evaluate against some gold standard

\subsection{Evaluation Data}

In order for any evaluation of the system's effectiveness as a geoparser to be made we must have some corpus of texts for which the results of geoparsing it are already known. I.e. we need a corpus of texts, each of which has all spatial references within it both recognised and associated with some physical location drawn from a gazetteer. Such a corpus will be the "gold standard" data, and for the purposes of the evaluation is taken as containing the correct results to the geoparsing problem. The geoparser system can then be run against the gold standard data (with all annotations stripped from it), and the results compared against those given by the gold standard data's annotations.

As this form of data is fairly particular to the task of geoparsing, there is not a great deal of data available that fits the criteria and would be suitable as the gold standard.

One possibility would to manually annotate some texts ourselves to be used as the gold standard. This has the advantage that the scope of the annotations used, such as the type of spatial references recognised and the gazetteer used to draw candidates from, could be chosen with particular consideration to the system being evaluated, so that only the same types of spatial references as recognised by the system could be annotated and the same gazetteer could be used. However this option would be very time consuming to produce an adequate quantity of annotations, which would not be a good use of the limited time available for the project; this is especially true given my lack of experience with annotating text in this way. 

An alternative option is to use some gold standard data created for the evaluation of a similar geoparsing system, for which there is some suitable data available. One data set in particular that is similar to the ideal required is the ACE 2005 English SpatialML Annotations corpus \footnote{https://catalog.ldc.upenn.edu/LDC2008T03}, created as part of the SpatialML project at the MITRE Corporation \citep{mani2010}.

The SpatialML corpus consists of 428 English text documents [TODO more details on stats], from various sources including news articles and Usenet discussions, hand-annotated with the SpatialML annotation scheme, and intended for use in the SpatialML project as training data.

However there are several problems that need to be overcome with using the SpatialML corpus as the gold standard for the evaluation process. Firstly, the SpatialML annotation scheme is designed to mark-up many types of spatial reference beyond just the named location references considered by our system. These include relational references such as "5 miles east of Sheffield" and nominal references such as "a city".

As such, if the annotated corpus was compared directly against the results achieved by our system running on the un-annotated corpus, the result would be very unfavourable to our system as it would not identify many types of spatial references which it had not been set up to recognise.

To remedy this, some amount of pre-processing of the corpus is required in order to discard those references which we are not considering.

In addition, though the SpatialML corpus associates each PLACE tag annotation with a gazetteer entry using an id from the gazetteer, the gazetteer used is the IGDB, which is neither freely available nor has been used for this project. This therefore presents difficulties with comparing whether a spatial reference from the corpus has been identified as the same location as the same spatial reference found by the system, as a different gazetteer id for a different gazetteer will be used, even if a reasonable person would consider the entries in each gazetteer as referring to the same place.

However this is not as big a problem as it may appear, as other information is given in the PLACE tags in the corpus in addition to the gazetteer id, including the latitude and longitude and the type of location, for the entry identified in the gazetteer. As such we can use other means of determining if two identified spatial references should be considered the same. For instance a simple but likely reasonably accurate test would be to consider them the same if both are identified as the same type of location, and they are within some threshold distance of each other based on their coordinates (as both gazetteer entries are most likely referring to the same location). Note that a threshold distance should be used rather than exact equality as any location occupies an area, but in both the GeoNames gazetteer and in the corpus PLACE tags only a single pair of coordinates is given for a location within this area. [TODO for added accuracy] 

% To evaluate need some gold standard
% - spatialml contains info needed but also extra information
% - not perfect: need to pre-process to use, contains reference to gazetteer not publically available

% - standard process for evaluating in information retrieval 

% Evaluation process - F-measure
% - what is and why used

\subsection{F-Measure}

[TODO have some refs would be useful here, add - need some ref at least]

Once we have the SpatialML corpus in a suitable format to be used for our evaluation, we then need to perform the comparison between this and the results of our system in a meaningful way. The standard accuracy measure used in information retrieval is the F-Measure, which is the harmonic mean of the precision and recall of the system (note that geoparsing is a type of information retrieval task, as the ultimate aim is the retrieval of as accurate a set of gazetteer locations as possible given a particular document, as judged against the set of locations the document's author intended).

The F-measure is therefore defined as

\begin{displaymath}
	F_{1} = 2 \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}}
\end{displaymath}

where precision is the proportion of retrieved results that are relevant (i.e. ratio of correct results over all results returned), and recall is the proportion of relevant results that are retrieved (i.e. ratio of correct results over all results that should have been returned).

The F-measure is favoured as a way for evaluating information retrieval systems as it tends to favour balanced systems, or systems that have both reasonable precision and reasonable recall, over imbalanced systems which have a good score for just one of these. Being calculated using the harmonic mean also means it will tend towards the lower of these and so ensure both precision and recall must be considered in order to obtain a high F-measure.

By contrast, if a simple arithmetic mean was used as the measure of a system's accuracy, consider a (completely useless) system which returned every possible result for any input. Such a system would have a recall of 100\% as all correct results will always be returned, and a small but non-zero precision as some results will still be relevant. The system would therefore already have an arithmetic mean of over 50\%, without any filtering of the results to return being done. Using the F-measure the system would instead get a very low score, as it should do.

For a geoparser system in partcular we consider three main aspects worth evaluating. First there is the overall score of the system, the accuracy of the system at both recognising the right sections of text as spatial references, and of associating these with the correct geographic location. For this the precision will be the proportion of spatial references correctly recognised and identified out of the total number of spatial references recognised and identified. The recall will be the proportion of spatial references correctly recognised and identified out of the proportion that could possibly have been recognised and identified.

It would also be useful to be able to evaluate the recognition and identification aspects of the system independently of each other. For the recognition the F-measure can also be used, with this time the precision being the proportion of spatial references correctly recognised as being a spatial reference (regardless as what gazetteer entry they are later identified as) out of the proportion of spatial references recognised, rightly or wrongly; the recall is the proportion of spatial references correctly recognised, out of the proportion that could possibly have been recognised.

For evaluating the identification component independently of the recognition component we should only consider those spatial references that have been correctly recognised, and consider how well the system can identify these. As such it does not make sense to consider the recall, as there are not results that should have been returned but were not; instead it makes sense to just consider the precision, i.e. of those spatial references correctly recognised what proportion are identified correctly.

There is one final point worth considering about the precision, recall and F-measure. When performing an evaluation in which there are many similar datasets and we wish to produce overall scores for the accuracy over all of these datasets, there are two alternate ways that an overall precision and recall, and therefore F-measure, can be calculated.

Firstly there is the macro-average versions of these figures, found by finding the precision and recall of each individual dataset and averaging each of these, and then using these averages to calculate the overall macro-average F-measure. Alternatively there is the micro-average method, which involves finding the precision and recall by treating all of the datasets as a single dataset.

The macro-average method is suitable for seeing how the system performs overall across the datasets, with each dataset given an equal weight in the result; however this therefore means that with this method a dataset which contains one point of data is given as much weight as one with many.

The micro-average method on the other hand is better suited when the datasets are largely similar and we do not care about the individual performance of the system on any single dataset but rather the data generally; it is also better suited when the datasets can vary greatly in size as it is just concerned with the individual data points rather than their sources. For these reasons the micro-average method is better suited to our evaluation, as we are concerned with our system's overall performance on the corpus generally. Additionally the SpatialML corpus documents are of greatly varying lengths and amounts of spatial references within them, and so the micro-average is also a better choice as otherwise this would influence the evaluation result.

% [TODO an end of section] 

%In other words this means that precision is the proportion of 

%Specifically, this means that these are defined as

%\begin{displaymath}
%	precision = \frac{\mathrm{true positives}}{\mathrm{true positives} + \mathrm{false positives}}
%\end{displaymath}


% Once got gold standard data can evaluate various aspects of system - recognition and identification, details of impl and results done in evaluation chapter

\chapter{System Design and Implementation}
%Outline chapter: describe in detail process of medium to low-level design and implementation of system

In this chapter we describe the implementation of the geoparser system and each of its components, as well as the low-level design choices involved in this implementation.

As described in \ref{overall_architecture}, we try to compartmentalize each major area of the system into distinct components for better modularity. The main logic of the program is contained within the \verb#map_locations.py# file, which includes the \verb#map_locations# function which performs the entire pipeline of the system, making calls to the other modules as needed and printing the current status of the system. Running this module is also how the system is run, by reading command-line arguments given and then executing the \verb#map_locations# function in accordance with these, and it is this file which we mean by the overall system.

The overall pipeline of the system begins by reading the command line arguments given when executing the \verb#map_locations.py# file, in which at least one of the \verb#url# or \verb#file# options must be given with an argument specifying where input should be obtained from, either a URL pointing to a HTML web page, or a local file path should be given. There is also a \verb#nomap# option, which disables the automatic viewing of the final map output (while still creating it for later viewing), and is useful for batch processing.

The input is then processed through each of the components of the system in turn, as described in the system design, with each stage of intermediate output being saved locally for later examination if needed. The final output of the system will then be the map result, and this will be automatically displayed in the browser.

\section{Content Acquisition}
\label{content_acquisition}
% Means of text entry to system and reasons for choice. Process and problems in extracting text from arbitrary website

Before text can be processed by the system it must be input in some way, which two alternate methods are available for. First of these is simply to give input text by providing a file which the text is to be read from; this is done using the \verb#file# command-line option. This option is useful both for processing a single file and for batch-processing a number of files, such as is done as part of the evaluation process; it is also useful when batch-processing files to use the \verb#nomap# option so that the map produced as the output is not shown when the process is finished.

In addition to file input it is also possible to give a URL which the input text is to be read from, using the \verb#url# option instead. This option is particularly useful for practical use of the program, as the web is frequently the source of new reading that users may come across, and the provision of a map to accompany this reading could be useful for their understanding of the material.

However processing text from the HTML at a given input URL is somewhat less simple than simply reading all of the text from a text file, due to the structure of the HTML. When a user gives a particular URL that they want processed, it can reasonably be assumed that they want only the main content at that URL to be processed and not any other small extraneous sections of text. However for most web pages with more than very minimal HTML, and especially for websites where the geoparser would be most useful such as news sites, there is a considerable amount of other text on the web page for any particular article, for example links to other parts of the site, small parts of other articles, reader comments and adverts. As such, simply stripping all HTML from a web page before processing it would always leave a considerable amount of extra material in the resulting text, which could pollute the input with unwanted text and so effect the output. This is especially likely since many of this other text could also potentially contain spatial references, for instance in the titles of other articles on the site.

As such some degree of intelligent parsing of the HTML is needed in order to make this a useful feature. One obvious solution would be to manually specify a specific parsing of the HTML for a given page in order to extract only the main content. However this would need to be done separately for every web site before the system could parse it and would also be easily broken by even small changes to the site's structure.

An alternative solution is to develop a general approach to extracting the main content from web pages, by following rules as to how pages are laid out generally rather than trying to parse each particular page separately. One way this could be done is by using the Readability Parser REST API \footnote{https://www.readability.com/developers/api/parser}, a freely available API which works very well for extracting the main content from a wide variety of web pages, and so rather than attempt to reimplement this we use this API  to perform this task. The API is accessed in the \verb#readability_interface# module, using the Requests \footnote{http://docs.python-requests.org/en/latest/} Python library to make HTTP requests for an article's content. The article content is then obtained from the JSON response returned. 

\section{Spatial Reference Recognition}
% Reasons for use of Stanford CoreNLP - do above in background

Once textual content has been entered into the system, the next stage of the process is the recognition of the spatial references in that content. As described previously, we concentrate on just the recognition of the named locations, using the NER component of Stanford CoreNLP for this purpose.

Though we are initially just using Stanford CoreNLP for this component, there is still some work needed to incorporate this into the overall system. CoreNLP \citep{manning2014} consists of a number of steps common to NLP, some set of which can be run as a pipeline to process a file through the full series of steps and finally retrieve the input file marked up with the output of all of the steps. These steps include simple, essential NLP tasks such as tokenization (the splitting of text into some tokens, which are some important components of the text such as words and punctuation) as well as more specialized tasks such as NER and coreference resolution (the identification of expressions which refer to the same entity).

CoreNLP can be accessed through both a Java API and as a command-line program, which can be run on a file with some arguments given in order to specify a CoreNLP pipeline to run on that file. Since it can be run from the command-line, we initially decided to simply run the CoreNLP pipeline on some text by running this command from within Python, in order to obtain the resulting text with named entities recognized.

However we ran into issues with running CoreNLP in this way. The NER component of CoreNLP requires trained models created by a machine learning process in order to run. Developed models are included for good NER of various standard categories in English, in particular a model is included for the recognition of Persons, Organizations, and Locations, trained on a number of datasets, and it is this which we use. Before this step can be run however the model must be fully loaded into memory; on the development machine this takes around a minute to complete. As the CoreNLP process is simply being run from Python as the command-line interface to the program, which can only process a single file at a time, this loading must be performed each time a file must have the NER component run on it, i.e. each time the geoparser is to be run. This is therefore an unacceptable amount of time to be added to the process each time the geoparser is run, and if possible some alternative way is needed to interface with CoreNLP.

A number of alternative ways to interface with CoreNLP were considered. Various third-party wrappers are available for CoreNLP to enable it to be used from languages other than Java, in particular from Python. NLTK, a natural language toolkit for Python, includes an interface to using Stanford CoreNLP \footnote{http://www.nltk.org/api/nltk.tag.html\#module-nltk.tag.stanford}, however this also functions by simply running the CoreNLP commands from the command-line, and so offered no improvement in speed.

We also tried using a Java Servlet wrapper available for just the Stanford NER \footnote{https://github.com/dat/stanford-ner}; this allows the NER process to load the models to memory once and then run continuously, waiting for HTTP requests for texts to be tagged and then responding to these. By using this we could interface with the CoreNLP process by making these requests from Python, without the process needing to be restarted and the models reloaded each time. This functioned sufficiently for just tagging the named entities in text, however we decided it was preferable to have the results of the other steps in the full CoreNLP pipeline, such as tokenization and co-reference resolution, also included in the output received so that these could later be used by the rest of the system if needed. For instance the co-references or some number of tokens near to a particular spatial reference could potentially be used to help with the disambiguation process.

Rather than attempting to learn and use other interfaces to CoreNLP, which would be more robust but also more complicated for our purposes than necessary, we instead created our own similar Java Servlet interface to the whole CoreNLP pipeline needed. This simple interface is in \verb#CoreNlpServlet.java#, and is intended to be run on a Apache Tomcat Server \footnote{http://tomcat.apache.org/}. Once this is run the CoreNLP pipeline for all steps we require will then persist and we can process text by making HTTP requests to the Servlet from Python; this is done from \verb#corenlp_interface.py#.

% Problems with interfacing with CoreNLP, problems with using various wrappers and eventual solution

% Include brief description of how Stanford CoreNLP ner works/other techniques? - TODO above

\section{Spatial Reference Identification}
\label{sec_spatial_reference_identification}
% Describe process needed to correctly identify a spatial reference

The next stage in the process is the identification of each of these recognised named locations, by associating each with at most one entry in the GeoNames gazetteer which it is deemed most likely to be. A spatial reference could potentially be associated with no GeoNames entry if no reasonable candidates can be found for it, or if a suitable amount of confidence cannot be reached for any of the candidates found, according to the disambiguation procedure. There are two stages to the identification process - the obtaining of all potential candidates and then the disambiguation procedure used to select the most likely to be true of these, if any - and we describe each of these in turn.

\subsection{Obtaining Candidates}

Once a gazetteer has been selected to obtain candidates from, the obtaining of candidates for any given recognised named location should be a reasonably straight forward process.

The entire identification process takes place within an \verb#identify# function in the \verb#identification# module. This function is input some text with the named entities and other aspects recognised in the format of Stanford CoreNLP, as well as a disambiguation function for the disambiguation of these named entities, and returns the GeoName locations identified from the tagged document according to this disambiguation function.

First of all the named locations in the document must be extracted, this is done by creating a \verb#LocationReference# object (from \verb#models.py#) for each such reference, for holding all information from the reference's context needed for the disambiguation process.

Candidates are then found for these reference objects, using their \verb#find_candidates# method. This method functions by making a connection to the MySQL GeoNames database, and then searching the main \verb#geoname# table of this for all candidates with a similar name as the spatial reference.

The search for a similar name includes all entries whose letters match except for certain additional aspects such as accents and capitalization. This is done as otherwise, for example, a search for "LONDON" would not include results with the name "London", even though these should clearly be included.

However just searching for locations based on their name against the names in the \verb#geoname# table frequently meant that the actual \verb#geoname# candidate  which matched the named location was not included. This is because in the \verb#geoname# table only a single name for each location is given, with the official name being preferred. However the same real location may have several names associated with it, both from different languages and alternate names within the same language. For instance, in the \verb#geoname# table the only reference to the country commonly known in English as Argentina has the name "Argentine Republic", as this is the official name for the country, and so the correct entry for this location is not included when a search for locations with the name "Argentina" is made.

Fortunately the GeoNames database also includes an \verb#alternate_names# table, containing all alternate names known for any of the entries in the \verb#geoname# table. As such, by searching this table as well for name matches and obtaining the corresponding \verb#geoname# entry, we can ensure all GeoName entries with a given name are included in the list of candidates found for a location.

For each result in both searches a \verb#Geoname# object is created, and the list of all of these objects is returned as the list of candidates.

Finally, all of these database queries were initially quite slow due to the large size of the GeoName database, and so indices were added to the database to optimize the making of these queries.

%It is also useful as it may often be the case that in certain situations, such as in a less formal piece of writing, accents are sometimes left off 

\subsection{Disambiguation}
% What it means to disambiguate a locational reference

Once these candidates have been obtained, a disambiguation algorithm function must then be run on them. This is some function which takes a list of candidate \verb#Geoname#s, and returns the top candidate of these according to its algorithm.

Unfortunately due to time constraints we were only able to implement a simple disambiguation algorithm which returns the candidate with the highest population as given in the gazetteer, a simple base measure of establishing a likelihood for a candidate to appear in a document.

Of course this algorithm is trivially simple, as it only takes account of one measure given in the gazetteer, without regards to the context of the spatial reference or other spatial references in the document. This also means that by this procedure any equivalently named spatial reference will always be identified as the same candidate, regardless of any context that might make it simple to correctly identify a reference. For instance by this procedure, "London" in "London, Ontario" will always be identified as London, UK, even though this is incorrect and would be simple to fix by a simple heuristic of having a comma indicate containment.

In addition to this we also implement an algorithm involving a simple random selection of a candidate, in order to form a simple means of comparison for how successful the other algorithm is. Of course, even more so than the previous one this method is trivially simple and of no practical use.

It would have been interesting to develop this component of the system further by considering some more interesting disambiguation algorithms. Unfortunately this was not possible due to time constraints, these occurred due to other essential aspects of the project taking longer than expected due to lack of experience with many aspects of the project. However we do discuss extensions to the disambiguation component that could be made and several possible improved disambiguation algorithms in \ref{subsec:improvements_processes}.

\section{Display of Results}
% Description of KML and reason for generating

Finally, once identified locations have been found for a particular document, we have the final component of the system, for plotting maps of these locations. This is a fairly straightforward process, involving first the generation of a KML file describing the map to be plotted, and then the generation of a HTML file to view the map specified.

The generation of KML is done by the \verb#kml_generation# module, using the lxml \footnote{lxml.de/} Python module for the generation of the XML required.

For each identified location from the text we want to plot a marker on the map, on the coordinates of that location. This can be done in KML by creating a "Placemark" element for each location. Coordinates can then be added to this element; we also add text for each marker to appear above it when clicked, this is the name of the spatial reference as given in the text, with the main name of the identified location from GeoNames given in brackets (which can be different due to finding candidates by alternate names also).

A HTML page is then generated, to display the map specified by the KML. This is done using the Google Maps API \footnote{https://developers.google.com/maps/web/}, a JavaScript API which can be used to include a Google map in a HTML document.

One issue encountered using just the Google Maps API was that though KML documents can be used to generate a map, they must be hosted publicly and so we would need to host them somewhere in order to this, requiring finding a way to do this and an extra uploading step to the process. However we found an alternative way to use the KML locally to create a map, using the third-party geoxml3 \footnote{https://code.google.com/p/geoxml3/} library which allows this to be done.

Using both of these APIs the final map viewing HTML document can be created. We generate these documents from Python by using a template \verb#map_view_template.html# page, and substituting the needed information such as the path to the KML file each time.

% Generating KML, HTML, and viewing in web browser (mention use of third-party library as can’t reference local KML document with Google maps)

\chapter{Results and Evaluation}
% Outline chapter: creation of evaluation setup and use to evaluate system

In this chapter we evaluate our geoparser system produced, both by considering the results produced for standard input and quantitatively according to the evaluation procedure. We begin in section \ref{sec:results} by considering the final map output produced by the system, and how effective the system seems at achieving its aims as set out in \ref{sec:requirements}. We then briefly describe in \ref{sec:creation_evaluation_environment} the creation of the evaluation procedure as described in \ref{evaluation_procedure}, before discussions the results given by this procedure in \ref{sec:evaluation_results}.

\section{Results}
\label{sec:results}

[TODO intro to this section and previous]

In this section we discuss the output produced by the system for a number of sample inputs. Though not quantitative this discussion is still useful as the output gives a visual indication of how well the system is performing, and can help give us insight into areas the system could be improved in.

Figures \ref{fig:ukraine_result} through \ref{fig:us_result} show the resulting maps from running the system on the URLs to three different news articles, each from a different source and on unrelated topics. These images show the standard output produced by the system, demonstrating that the system can fully process HTML articles from the web through the pipeline described in \ref{overall_architecture}, and output a map containing the named locations given in those articles.

The maps produced also demonstrate between them a number of common mistakes that the system can make during the processing through this pipeline, which it is useful to observe to provide areas that the system could be improved in.

First among these errors it can be observed in figure \ref{fig:iraq_result} that "US", referring in the article to the country the United States of America, has instead been identified as Us, France. This is indicative of a limitation of the GeoNames gazetteer, as the United States of America is not included in the candidate results when "US" is searched for, due to "US" not being included amongst the alternative names for the country. As such the disambiguation process can only select from among the other location entries returned, of which Us, France is returned by the simple disambiguation algorithm used due to having the highest population from amongst these.

Of course this error could easily be fixed by the inclusion of the United States in the list of candidates returned when a query is made for "US", by adding this as an alternate name to the gazetteer. However it also indicates the general importance of a good gazetteer for obtaining candidates from as, no matter how good the disambiguation algorithm used is, if the correct result is not included in the candidates returned then it can never be found as the final result.

Additionally this also suggests that there may be benefit to making the obtaining of candidates from the gazetteer more permissive, such as including some form of fuzzy name matching when searching. This would improve the chances of the correct gazetteer entry actually being returned amongst the candidates selected and so, coupled with an effective disambiguation algorithm which could choose effectively from amongst more candidates, could improve the overall accuracy of the system.

% US identified as Us in France
 

% iraqi identified as location by CoreNLP

Another error that can be observed, also in figure \ref{fig:iraq_result} is that the nationality "Iraqi" mentioned in the article has been misidentified as referring to the location of "Irāqī, Afghanistan". This is due to Stanford CoreNLP recognising "Iraqi" as a named location, and so it is included in the spatial references passed on to the identification component of the system. The identification component then processes this wrong result as best it can and associates it with a location which happens to have this name, and in this way a location never mentioned is included in the system's output.

This error shows how even the results of Stanford CoreNLP have some room for improvement, and could potentially be improved in order to give this component of the system better results. Both this error and the previous error also demonstrate how errors can propagate through a pipeline system such as this, with an error at one stage in the process generally being passed on through the pipeline to give spurious results.

Finally two other example errors can be seen here, due to the limitations of the disambiguation process. In figure \ref{fig:iraq_result} a mention of "Arlington, Va." has been misidentified as Arlington, Texas; meanwhile in figure \ref{fig:us_result} "Nineveh", Iraq has been misidentified as Nineveh, Nova Scotia, Canada.

Both of these errors are examples of obvious errors in the disambiguation process, which by observation could easily be fixed by using a more effective technique. For instance, from its immediate context it is obvious that "Arlington" here refers to an Arlington in Virginia; similarly "Nineveh" is mentioned in an article which mentions many other locations in Iraq, and so a more effective disambiguation process would take account of this to favour candidates in Iraq to some extent. 

% Nineveh identified as in US, Arlington identified as in 


\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{ukraine_result}
  \caption{Resulting map of running system with URL http://www.theguardian.com/world/2014/jul/01/ukraine-petro-poroshenko-goes-on-attack}
  \label{fig:ukraine_result}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{iraq_result}
  \caption{Resulting map of running system with URL http://www.bbc.co.uk/news/world-middle-east-29098099}
  \label{fig:iraq_result}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{us_result}
  \caption{Resulting map of running system with URL http://www.washingtonpost.com/sf/investigative/2014/06/22/crashes-mount-as-military-flies-more-drones-in-u-s/}
  \label{fig:us_result}
\end{figure}


\section{Creation of Evaluation Environment}
\label{sec:creation_evaluation_environment}
% Process of adapting SpatialML corpus to be used for evaluation, and creating evaluation script
% Possible problems with evaluation process

We will now move to a more quantitative discussion of the accuracy of the system, using the evaluation procedure as described in \ref{evaluation_procedure}. Before we do this however we briefly describe the process of creating the evaluation environment for the system.

The first thing needed for the evaluation process is the transformation of the SpatialML corpus to make it suitable for use as the gold standard data in the evaluation.

As described previously, this is required as it includes the mark-up with PLACE tags of more types of spatial references than our system, which only identifies named locations. Without pre-processing to make the corpus only contain references of an equivalent form as our system identifies, our system would be penalized for not recognising spatial references of types which it does not attempt to recognise.

The SpatialML corpus also contains other tags, such as LINK and SIGNAL tags, for the mark-up of other aspects of the text, and these are also removed as they are unnecessary for our evaluation.

The pre-processing of the corpus is performed by the \verb#transform_spatialml_corpus.py# script, and is generally fairly straightforward. The different types of spatial references tagged are mostly easy to distinguish, as nominal reference PLACE tags (such as of "a city") have a "form" attribute with value "NOM", and predicative PLACE tags (such as of "Japanese") have a "predicative" attribute with value "true". These can therefore easily be removed from the corpus.

A version of the corpus was also created with all tags stripped, using \verb#strip_spatialml.py#, to be batch processed by the system to obtain the system's results for processing the corpus.

The evaluation process itself is created in \verb#evaluation.py#. To run the evaluation process on the geoparser using a particular disambiguation algorithm, the geoparser must first batch process the raw SpatialML texts through the entire pipeline, and the resulting output locations should be stored. The evaluation process then runs by, for each document in turn, comparing the locations identified by the system against the gold standard locations.

%by taking all of the identified locations, and comparing them for each document in turn against the gold standard locations found for that document.

For each document, the set of identified locations is compared against the set of gold standard locations, and a number of running totals are incremented appropriately.

Running recognition totals for the true positives, false positives, and false negatives of spatial references recognised by the system are maintained; a recognised spatial reference is only counted as a true positive if it precisely matches a span tagged as a PLACE in the SpatialML corpus.

Similarly a running total is kept for the true positives, false positives, and false negatives for locations identified through the overall system. The count is only made for those spatial references in the corpus that it is possible  to tell if the system has identified correctly. This is done by excluding from consideration those spatial references in the corpus for which no coordinates are given, as since different gazetteers are used for the corpus and the system there is no way to determine if two spatial references should be considered the same without coordinates.

A distance threshold value of 100 kilometres between their coordinates is used to determine if two spatial references should be considered identified as the same location. This value was chosen by considering a range of values, and smaller values were found to greatly decrease the number of true positives for identified locations while larger values led to little increase. It was also judged to provide a good balance as in most cases which will be encountered in the corpus any two locations with the same name and within 100 kilometres of each other will likely be referring to the same location. Any value chosen as a threshold will always be somewhat arbitrary, however it is necessary to choose some number to be able to make the decision of when identified and corpus locations should be considered the same.

The shortest distance between the two coordinates can be calculated by calculating the great circle distance between them, as defined by \citet{matuschek2014}, and they are considered equivalent if they are within this threshold distance of each other.

Finally a running total of the true positives and false positives for just the disambiguation process is made also; locations are considered correctly disambiguated by the same threshold criteria as for the overall pipeline. We consider only those spatial references which it would be possible to disambiguate, and so only those which have already been correctly recognised. In addition as for evaluating the overall pipeline we only consider those spatial references which have coordinates associated with them in the corpus and so it is possible to tell if they have been correctly identified.

From these running totals the micro-average precision, recall, and F-measure for recognition and the overall pipeline, and for just precision for disambiguation, can be calculated.

\section{Evaluation Results}
\label{sec:evaluation_results}
% Description of each disambiguation algorithm, figures and graphs of results of evaluation, comparison between and reasons for difference

% TODO put in how the random alg gives dif result each time

We now consider the results produced by running the evaluation procedure on the geoparser. 

\begin{table}[H]
	\centering
	\begin{tabular}{| c | c | c |}
		\hline
		Precision & Recall & F-measure \\ \hline
		82.7\% & 73.7\% & 77.9\% \\ \hline
	\end{tabular}
	\caption{ Evaluation results for the recognition of spatial references, using Stanford CoreNLP. }
	\label{table:recognition}
\end{table}

Table \ref{table:recognition} shows the results of the recognition component of the evaluation procedure, i.e. the effectiveness of Stanford CoreNLP's NER component at the recognition of the named locations in the SpatialML corpus. As can be seen the results are reasonably good, as should be expected from Stanford NER as a state of the art NER system [TODO find other figures for CoreNLP and contrast]

\begin{table}[H]
	\centering
	\begin{tabular}{| c | c |}
		\hline
	    & Precision \\ \hline
		Random & 30.7\% \\ \hline
		Highest population & 86.8\% \\ \hline
	\end{tabular}
	\caption{ Evaluation results for the disambiguation of recognised spatial references achieved using the highest population and random disambiguation algorithms. }
	\label{table:disambiguation}
\end{table}

Table \ref{table:disambiguation} shows the disambiguation results of the evaluation procedure, using both the random disambiguation algorithm and the procedure of selecting the location with the highest population.

The main thing to be noticed here is that both of these figures are quite high considering the simplicity of the techniques used. In the case of the random method, even though it involves the random selection of any of the candidates, it still achieves an accuracy of over 30\%. However this is simply due to the fact that for many of the spatial references in the SpatialML corpus there are only a few candidates in the GeoNames gazetteer, with often just one or two. Therefore it is only to be expected that a significant minority of the time this method selects the correct candidate.

In the case of the highest population disambiguation, this high accuracy indicates that the population is actually a fairly good baseline indicator of a candidates likelihood of being mentioned in a document. While it seems reasonable for this to be true, as population is a good indicator of a location's relative importance (at least for populated places), this figure is also somewhat misleading.

This is because more populated places will generally be referenced more often, particularly in news articles which the SpatialML corpus largely consists of; the same locations are also likely to be referenced frequently throughout the corpus. This means that though the disambiguation algorithm scores quite highly, this is in large part due to it recognising some of the same major locations repeatedly, while repeatedly misidentifying various more minor locations.

\begin{table}[H]
	\centering
	\begin{tabular}{| c | c | c | c |}
		\hline
		& Precision & Recall & F-measure \\ \hline
		Random & 24.1\% & 23.6\% & 23.9\% \\ \hline
		Highest population & 68.2\% & 66.9\% & 67.5\% \\ \hline
	\end{tabular}
	\caption{ Evaluation results for overall accuracy of entire geoparser pipeline, using the two disambiguation algorithms. }
	\label{table:overall}
\end{table}

Finally, figure \ref{table:overall}	shows the results of the overall evaluation of the system, for both of the disambiguation procedures. This is largely as would be expected by the combination of the recognition and disambiguation results. As can be seen the overall system scores reasonably using the highest population disambiguation algorithm, however there is still a significant amount of room for improvement.

[TODO contrast with other results, spatialML results + reasons similar to above]


% Possible other algorithms, factors which could be used

% How effective overall system is

% Possible improvements to system

\section{Future Work}

In this section we discuss future work that could be done on the subject of geoparsing. Future extensions to this project can broadly be split into two categories, although there is necessarily some overlap between these. Firstly there are improvements to the functions of a geoparsing system in terms of its use as a usable, practical system, better capable of realising the benefits a geoparser can provide as described in section \ref{sec:overview_georeferencing}. Then there are improvements that could be made to the individual geoparsing processes of the system, in particular improvements to the recognition and disambiguation techniques used. We discuss each of these areas in turn.

\subsection{Creation of a Practical Geoparsing System}

There are a number of extensions that could be made to the geoparser system to make it a more robust, usable, and functional system. However rather than extending the existing system, one option that could be considered is the extension of existing open source projects.

In section \ref{subsec:research_systems} we describe a number of previous geoparsing research systems, which share a considerable amount of overlap in  their functionality. Rather than the continual reimplementation of similar geoparsers in order to explore new research directions, one useful contribution to the field of geoparsing would be the creation of a reference geoparsing system.

In particular this system could be built upon the open-source CLAVIN project, described in section \ref{subsec:clavin}. Additional features could be added to this system to make it a full and usable geoparsing pipeline, such as adding a full interface and the generation of map output of the locations recognised by the system. 

A reference system such as this could then be used for the easy incorporation into other systems to allow greater access to geoparsing, and also allow the experimentation with just a single aspect of the geoparsing process, such as with novel disambiguation algorithms, without having to reimplement the rest of the pipeline.

%Some further directions the project could be taken in is the creation of specialized systems for situations where a geoparser may be of 

% Would actually suggest extending CLAVIN for further dev of geoparser

% improve system in dif directions - GUI, browser plugin

% add polygon representations


\subsection{Improvements to Geoparsing Processes}
\label{subsec:improvements_processes}
% improve disambiguation algorithms

% use metadata as information for diambiguation

% add other types of spatial references


\chapter{Conclusions}
% To include: lack of SpatialML system documentation, gazetteer, corpus hampers future work

% consider success of the project as a whole, and conclusions can be drawn from it 

In this chapter we consider the overall 






\bibliographystyle{plainnat}
\bibliography{dissertation.bib}


\end{document}


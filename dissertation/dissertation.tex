\documentclass[12pt, a4paper]{report}
\usepackage[round]{natbib}
\usepackage{glossaries} 

\title{Mapping Spatial References in Text using Google Maps}
\date{September 10, 2014}
\author{Bob Whitelock\\ MSc Software Systems and Internet Technology\\ Department of Computer Science\\ The University Of Sheffield}

%\makeglossaries
%\newglossaryentry{georeferencing}
%{
%	name=georeferencing
%	description={todo}	
%}


\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\tableofcontents

\glsaddall
\printglossaries

\chapter{Introduction}
% Briefly set scene of background to project

% Outline aims, objectives of project

% Summarize remaining chapters


\chapter{Literature Review and Previous Work}
% Overview of chapter contents and reasons structured this way




% Review previous work and relationship to own
% Identify general trends and positions in area
% Overview of other available systems

% Background to georeferencing systems
\section{Overview of Georeferencing and Geoparsing}
% Overview of what is involved in a georeferencing system

% TODO redo this section

The aim of this project is to produce and evaluate a prototype geoparsing system; a system for the recognition and identification of spatial references in text. The system will also include means of inputting text to be parsed and of viewing the output of the system on a map. In this section we give an overview of geoparsing and the procedure involved, as well as outlining the benefits of both geoparsing and the wider field of georeferencing, the association of geographic information with text. 

*** FIX
\citet{hill2006} describes the importance of georeferencing of information as a means for assisting its understanding and interpretation. Increased georeferencing of information gives many benefits, as the understanding of information in terms of its spatial context  

Firstly this is due to the ubiquitous nature of spatial references in text, as well as the many benefits of increased access to this information  due to both the  Given the ubiquity of references to place in text,  

This importance stems from two directions. Firstly, it is worth noting that all events occur in space and time, and so improved access to spatial (and temporal) information can aid in the visualization and understanding of such events. In addition, it is also clear that "[g]eoreferenced information is everywhere" \citep{hill2006}, for instance: "[i]t has been estimated that at least 70 percent of our text documents contain placename references" (MetaCarta Inc., 2005a, cited in \citet[p.~5]{hill2006}). Together, these points indicate that greater georeferencing of information can lead to improved understanding of information in a wide variety of areas...
***

Geoparsing in particular is a subject that both improved techniques and wider adoption would be of great use in many areas of information retrieval and and analysis, especially given the ever-increasing number of digital documents available on the internet. The development of improved geoparsing software and the wider use of this could have applications in many areas, both in simplifying current tasks and enabling new actions to be performed that would be infeasible or impossible otherwise. For example, a geoparsing desktop application,  browser plugin, or website could allow users to easily visualise the geospatial events in any document they are reading; similarly such software could allow content creators to easily create a map of events in some content to accompany it, automating a task which would previously only be done manually if at all. In addition the use of geoparsing software by either content creators or search providers could allow improve the searching of content by geographic location, for instance by allowing the returning of relevant documents that contain references to locations in some area but do not reference the area itself. Furthermore, geoparsing of large bodies of text could enable analysis that would be impractical otherwise, such as analysing the spatial references in a large number of historical records.

\citet{hill2006} gives a detailed breakdown of the general steps involved in the geoparsing of a document, and these are as follows:

\begin{enumerate}
	\item {Digital text to be parsed is input to the system, and natural language techniques are used to identify features useful to the identification of spatial references, such as location names (by named-entity recognition), feature types (such as words like 'city' or 'mountain'), and other features of the text which may be useful (such as words indicating a relationship like 'in' or 'near' or directional or distance indicators like 'north of' or 'fifty miles east').}
	\item {The output of the initial step will be a number of spatial references, including but not necessarily limited to location names, and associated information from the text, and these should then be used to search for possible candidates for identification for each spatial reference, by lookup in a gazetteer (a geographic database containing location names as well as various associated information such as type of feature and geographic coordinates).}
	\item {If the gazetteer lookup is successful for a particular spatial reference then there will one or more candidates in the gazetteer for the identification of this reference.}
	\item {If more than one candidate has been found, an attempt must be made to disambiguate the most likely candidate to be that which the spatial reference is actually referring to; various techniques can be used for this, and both the various clues from the rest of the document and any other information given in the gazetteer can be used as factors in the disambiguation. If no candidates are found in the gazetteer for a spatial reference this disambiguation clearly cannot be done, and this is indicative of either limitations in the gazetteer or errors in the previous step of recognition of spatial references.}
	\item {Once a spatial reference has been identified as some gazetteer entry, coordinates can then be assigned to it with some degree of confidence. In addition, if desired an overall set of coordinates can be assigned to the document to georeference it as a whole; this could be done by different techniques, such as by an average of the individual identified coordinates or by identifying the location the document is primarily discussing and using the coordinates associated with this.}

\end{enumerate}


\section{Named Entity Recognition}
% What is involved, state of the art, Stanford CoreNLP in particular

One of the central components of a complete geoparsing system is a mechanism for the identification of spatial references in the text to be parsed. The key part of this component will be a means of recognising location names, as these are the most prominent type of spatial reference in most text, and in many cases the only type it will be possible to identify definitively. This problem is a specific instance of the general problem of named-entity recognition (NER); in this section an overview of NER is given with a summary of some of the main techniques in the field.

The task of NER is the automatic identification of parts of text that are the name of some named-entity, such as of a person, a location, or an organization, as well as for the classification of identified named-entities into groups such as these. What precisely is defined as a named-entity is hard to define definitively, however for the most part the definition of \citet{website:conll2002} will suffice, that '[n]amed entities are phrases that contain the names of persons, organizations, locations, times and quantities'. NER broadly consists of two parts, firstly the recognition of a sequence of tokens as a named-entity, and secondly the classification of named entities into particular types. It is by no means a simple task for numerous reasons, to name just some there are many different situations in which a named-entity can appear, the exact same string can refer to one type of entity in some situations and another in a different situation (for example "England" could refer to the country England or the organization of the English football team, depending on context), and difficulties in determining what constitutes a single named-entity (for example 'The Royal Bank of Scotland', which would correctly be identified as an organization, could easily be interpreted as two separate entities, an organization and a location, separated by 'of', due to the emphasis usually placed upon capitalization in determining named entities).

NER has received a considerable amount of work over the past twenty years, with a variety of techniques developed, many with considerable success in the particular area targeted. All techniques use various features of both words and a document to try to determine both the extent of and classification of named-entities in the document, often by the development of rules for recognition and classification based on these features. Such features can be of a variety of types and can take different types of values, including a boolean value, any of a range of valid values, or any string; the common factor however is that they are unambiguously determinable by a computer for. As described by \citet{nadeau2009}, common features used for recognition and classification can include:

\begin{itemize}
	\item {Word-level features: features of individual words, including: case of word (upper, lower, starts with a capital, mixed); any and type of punctuation in the word; any and type of digits in the word; types of other characters in the word; morphology of the word (the prefix, suffix, whether singular or plural); part-of-speech (the type of word, whether a foreign word); the application of some function over the word (such as extracting the non-alphabetic characters of the word).}
	\item {List-lookup features: various external sources to be used to assist in the recognition and classification of named-entities, including: general sources, such as dictionaries (e.g. to identify common nouns), lists of stop words, lists of common capitalized nouns, and common abbreviations; lists of various entities, such as names or parts of person, organization, or location names; lists of cues for entities, such as words typically found in organization names, person titles or common prefixes, and common words found in location names.}
	\item {Document features: features of the overall context of a word, the whole document, or the corpus as a whole, including: features of other occurences of a word, such as their case, and properties of coreferences of a word; the local syntax of a word; any meta information of a document, such as the url or information from tables or figures in the document; the frequency of words and phrases occurring in the document, and how often certain words occur together.}
\end{itemize}

Approaches to NER then use these various features to try to identify and classify named-entities in particular documents, usually by the  development of both recognition and classification rules which, applied to a document, should identify and classify named-entities based on the features of the document. As summarized by \citet{nadeau2009}, early approaches involved the use of 'heuristics and handcrafted rules', specifically tailored by the creator to try to tackle typical situations, determined by inspection of text in the domain to be tackled. More recently, however, the most common approach is to use various supervised machine learning techniques to train either a rule-based system or a sequence labelling algorithm, using a large quantity of human-annotated text as the training data. Some techniques in this approach include Hidden Markov Models, Decision Trees, Maximum Entropy Models, Support Vector Machines and Conditional Random Fields; the details of the majority of these is beyond the scope of this section.

There are also other techniques which require less or no human-annotated text to perform; these are important areas of development due to the time and expense needed to annotate new datasets for training use in purely supervised machine learning approaches. One such approach is semi-supervised learning, the main technique of which is known as 'bootstrapping'. This involves the use of a smaller dataset than with a purely supervised learning approach as seed example data to start the learning process, and then searches for similar examples to those in this set to reinforce the learning; some such approaches have had success close to a purely supervised approach. Finally there are unsupervised approaches, the main technique of which is known as 'clustering', which require no training data. These techniques mostly involve the use of lexical patterns and statistics on an unannotated corpus to solve the NER problem.

Of course location names are not the only type of spatial reference


\section{Geoparsing Systems}
% Discuss various georeferencing systems and algorithms involved


\chapter{System Overview}
% Outline chapter: because more experimental project merge requirements and overview of design into this section, as requirements quite flexible and process more iterative than fixed requirements-design-implementation

\section{Requirements}
% Overview of requirements and aims of system in light of literature review

\section{Choice of Programming Language}
% Brief reasoning for choosing Python as main programming language for project

\section{System Architecture}
% Overview of architecture of system as series of components with overall pipeline through components, reasons and purpose for design

\section{System Evaluation Procedure}
% Description of procedure to be used for evaluating the effectiveness of the overall program and disambiguation strategies


\chapter{System Design and Implementation}
%Outline chapter: describe in detail process of medium to low-level design and implementation of system

\section{Content Acquisition}
% Means of text entry to system and reasons for choice. Process and problems in extracting text from arbitrary website

\section{Spatial Reference Recognition}
% Reasons for use of Stanford CoreNLP

% Problems with interfacing with CoreNLP, problems with using various wrappers and eventual solution

% Include brief description of how Stanford CoreNLP NER works/other techniques?


\section{Spatial Reference Identification}
% Describe process needed to correctly identify a spatial reference

\subsection{Choice of Gazetteer}
% Factors involved in choosing a gazetteer and reasons for choosing

% Choice of interface with gazetteer – REST API vs. database

% Creating and indexing database - include footnote to script used

\subsection{Disambiguation}
% What it means to disambiguate a locational reference


\section{Display of Results}
% Description of KML and reason for generating

% Generating KML, HTML, and viewing in web browser (mention use of third-party library as can’t reference local KML document with Google maps)


\chapter{Evaluation}
% Outline chapter: creation of evaluation setup and use to evaluate system

\section{Creation of Evaluation Environment}
% Process of adapting SpatialML corpus to be used for evaluation, and creating evaluation script
% Possible problems with evaluation process

\section{Evaluation of Disambiguation Algorithms}
% Description of each disambiguation algorithm, figures and graphs of results of evaluation, comparison between and reasons for difference

% Possible other algorithms, factors which could be used

\section{Evaluation of Overall System}
% How effective overall system is

% Possible improvements to system

\chapter{Conclusions}
% To include: lack of SpatialML system documentation, gazetteer, corpus hampers future work

\section{Conclusion}

\section{Future Work}


\bibliographystyle{plainnat}
\bibliography{dissertation.bib}


\end{document}

